{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dear Student,\n",
        "\n",
        "In Homework 2, you will delve into the critical topic of imbalanced classification. Many real-world problems are, fortunatelly, inherently imbalanced. For instance, events like being diagnosed with cancer or experiencing a fatal car crash are relatively rare. Fortunately, only some percentage of patients visiting a dermatologist with suspicious moles are found to have cancerous ones. However, the significance of accurately identifying those cancerous cases cannot be overstated.\n",
        "\n",
        "Classification algorithms like neural networks are unaware of the importance of the minority class and focus solely on minimizing the loss. For example, if 99% of the dataset consists of negative class samples and only 1% represents positive class samples, a model could achieve 99% accuracy simply by predicting all instances as belonging to the negative class.\n",
        "\n",
        "In this homework, your task is to classify fraudulent credit card transactions. You are tasked with identifying the sampling algorithm that helps to achieve the highest accuracy in predicting fraudulent transactions.\n",
        "\n",
        "Credit card fraud detection dataset contains transactions made by credit cards in September 2013 by European cardholders.\n",
        "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. Here is the full description of this dataset:\n",
        "https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data"
      ],
      "metadata": {
        "id": "_rxhk4_55Jgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import shutil\n",
        "import os\n",
        "# Download latest version\n",
        "data_path = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\", force_download=True)\n",
        "fpath = os.listdir(data_path)[0]\n",
        "print(data_path)\n",
        "\n",
        "# Set the destination folder\n",
        "destination_folder = \"/content\"  # Replace with desired folder\n",
        "shutil.move(os.path.join(data_path,fpath), destination_folder)\n"
      ],
      "metadata": {
        "id": "FhFrjveSBFaR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "b405f371-a956-48dc-8c0a-3bf8c4fd899d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/mlg-ulb/creditcardfraud?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 66.0M/66.0M [00:02<00:00, 23.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.cache/kagglehub/datasets/mlg-ulb/creditcardfraud/versions/3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/creditcard.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "After downloading the dataset, we will load it using the pandas read_csv function. You can explore the features in the dataset. Most of these features are \"anonymized\" to protect clients' private information, and are transformed using the PCA algorithm, likely, to prevent reverse engineering.\n",
        "\n",
        "However, two features, \"Amount\" and \"Time\", are retained in their original form. The \"Class\" feature indicates whether a transaction is fraudulent or not and will serve as the target (labels) for the classification task."
      ],
      "metadata": {
        "id": "Z0JMAWMy-Fxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming the downloaded file is named 'creditcard.csv'\n",
        "df = pd.read_csv('/content/creditcard.csv')\n",
        "\n",
        "# Print some basic information about the dataframe\n",
        "print(df.head())  # Print the first 5 rows\n",
        "print(df.info())   # Print information about the columns and data types\n",
        "print(df.describe()) # Print descriptive statistics of the dataframe"
      ],
      "metadata": {
        "id": "Nm-DHN7lMj4I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe5a7b3-7027-41fc-b7f3-80dbf2b4f943"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
            "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
            "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
            "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
            "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
            "\n",
            "        V26       V27       V28  Amount  Class  \n",
            "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
            "1  0.125895 -0.008983  0.014724    2.69      0  \n",
            "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
            "3 -0.221929  0.062723  0.061458  123.50      0  \n",
            "4  0.502292  0.219422  0.215153   69.99      0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284807 entries, 0 to 284806\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    284807 non-null  float64\n",
            " 1   V1      284807 non-null  float64\n",
            " 2   V2      284807 non-null  float64\n",
            " 3   V3      284807 non-null  float64\n",
            " 4   V4      284807 non-null  float64\n",
            " 5   V5      284807 non-null  float64\n",
            " 6   V6      284807 non-null  float64\n",
            " 7   V7      284807 non-null  float64\n",
            " 8   V8      284807 non-null  float64\n",
            " 9   V9      284807 non-null  float64\n",
            " 10  V10     284807 non-null  float64\n",
            " 11  V11     284807 non-null  float64\n",
            " 12  V12     284807 non-null  float64\n",
            " 13  V13     284807 non-null  float64\n",
            " 14  V14     284807 non-null  float64\n",
            " 15  V15     284807 non-null  float64\n",
            " 16  V16     284807 non-null  float64\n",
            " 17  V17     284807 non-null  float64\n",
            " 18  V18     284807 non-null  float64\n",
            " 19  V19     284807 non-null  float64\n",
            " 20  V20     284807 non-null  float64\n",
            " 21  V21     284807 non-null  float64\n",
            " 22  V22     284807 non-null  float64\n",
            " 23  V23     284807 non-null  float64\n",
            " 24  V24     284807 non-null  float64\n",
            " 25  V25     284807 non-null  float64\n",
            " 26  V26     284807 non-null  float64\n",
            " 27  V27     284807 non-null  float64\n",
            " 28  V28     284807 non-null  float64\n",
            " 29  Amount  284807 non-null  float64\n",
            " 30  Class   284807 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 67.4 MB\n",
            "None\n",
            "                Time            V1            V2            V3            V4  \\\n",
            "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
            "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
            "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
            "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
            "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
            "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
            "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
            "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
            "\n",
            "                 V5            V6            V7            V8            V9  \\\n",
            "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
            "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
            "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
            "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
            "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
            "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
            "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
            "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
            "\n",
            "       ...           V21           V22           V23           V24  \\\n",
            "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
            "mean   ...  1.654067e-16 -3.568593e-16  2.578648e-16  4.473266e-15   \n",
            "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
            "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
            "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
            "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
            "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
            "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
            "\n",
            "                V25           V26           V27           V28         Amount  \\\n",
            "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
            "mean   5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619   \n",
            "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
            "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
            "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
            "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
            "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
            "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
            "\n",
            "               Class  \n",
            "count  284807.000000  \n",
            "mean        0.001727  \n",
            "std         0.041527  \n",
            "min         0.000000  \n",
            "25%         0.000000  \n",
            "50%         0.000000  \n",
            "75%         0.000000  \n",
            "max         1.000000  \n",
            "\n",
            "[8 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a proportion of fradulent transaction samples."
      ],
      "metadata": {
        "id": "HbngU3m0AaXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_count = df[\"Class\"].value_counts()\n",
        "print('Amount of samples in Class 0:', target_count[0])\n",
        "print('Amount of samples in Class 1:', target_count[1])\n",
        "print('Proportion:', target_count[1] / target_count[0], ': 1')\n",
        "\n",
        "target_count.plot(kind='bar', title='Count (target)')\n",
        "\n"
      ],
      "metadata": {
        "id": "2wzWPeU9M4K8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "139f6040-7a47-4530-8307-6a175c1f220c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amount of samples in Class 0: 284315\n",
            "Amount of samples in Class 1: 492\n",
            "Proportion: 0.0017304750013189597 : 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: title={'center': 'Count (target)'}, xlabel='Class'>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At first, lets set aside the test set. It is very important to set a test side aside before performing transformation to the dataset."
      ],
      "metadata": {
        "id": "lhcQ3tVYAmif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Separate features and target variable\n",
        "X = df.drop('Class', axis=1)  # Replace 'target_variable' with your target column name\n",
        "y = df['Class']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f'Train set: proportion of fraud/non-fraud {len(y_train[y_train == 1])/len(y_train[y_train == 0])}')\n",
        "print(f'Test set: proportion of fraud/non-fraud {len(y_test[y_test == 1])/len(y_test[y_test == 0])}')\n"
      ],
      "metadata": {
        "id": "Jn25YmAAT-Zj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d407b47-443d-4897-8a9f-c6a47c69ddc0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: proportion of fraud/non-fraud 0.0017322412299792043\n",
            "Test set: proportion of fraud/non-fraud 0.0017234102419808666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The anonimized features are already scaled but \"Time\" and \"Amount\" features are not. Let's scale those features. it should not effect scaling of other features."
      ],
      "metadata": {
        "id": "Uxi75rgfA20S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "N6mtEnlhcPXT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At first train the model on the original dataset. This would be our baseline, you would need to improve the performance upon it."
      ],
      "metadata": {
        "id": "cj6eahDaBPJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, initializers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def build_mlp_model_with_he_init(input_dim):\n",
        "    \"\"\"\n",
        "    Builds a Multilayer Perceptron (MLP) model with He Normal weight initialization for ReLU activation.\n",
        "\n",
        "    Args:\n",
        "      input_dim: The input shape (number of features).\n",
        "\n",
        "    Returns:\n",
        "      A compiled Keras model.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(64, input_dim=input_dim, activation='relu', kernel_initializer=initializers.HeNormal()),\n",
        "        layers.Dense(32, activation='relu', kernel_initializer=initializers.HeNormal()),\n",
        "        layers.Dense(1, activation='sigmoid')  # Sigmoid output for binary classification\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Assuming X_train and y_train are already defined from previous steps\n",
        "\n",
        "# Get the shape of the training data\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "# Create the model\n",
        "model_with_he_init = build_mlp_model_with_he_init(input_dim)\n",
        "\n",
        "# Compile the model with the Adam optimizer and binary crossentropy loss\n",
        "model_with_he_init.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Add early stopping to monitor validation loss and prevent unnecessary training\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model and monitor for overfitting\n",
        "history = model_with_he_init.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=10,  # Increase epochs to potentially observe overfitting\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping],  # Stop if validation loss stops improving\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model performance on the test set\n",
        "test_loss, test_accuracy = model_with_he_init.evaluate(X_test, y_test)\n",
        "print('Test Accuracy:', test_accuracy)"
      ],
      "metadata": {
        "id": "9SpomEy1cXwb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef4273e-cd68-4e90-b8a8-e821303ff2af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 4ms/step - accuracy: 0.9930 - loss: 0.0255 - val_accuracy: 0.9993 - val_loss: 0.0033\n",
            "Epoch 2/10\n",
            "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2ms/step - accuracy: 0.9995 - loss: 0.0033 - val_accuracy: 0.9995 - val_loss: 0.0028\n",
            "Epoch 3/10\n",
            "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0030 - val_accuracy: 0.9994 - val_loss: 0.0028\n",
            "Epoch 4/10\n",
            "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0027 - val_accuracy: 0.9994 - val_loss: 0.0026\n",
            "Epoch 5/10\n",
            "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.9995 - loss: 0.0023 - val_accuracy: 0.9993 - val_loss: 0.0035\n",
            "Epoch 6/10\n",
            "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0018 - val_accuracy: 0.9995 - val_loss: 0.0026\n",
            "Epoch 7/10\n",
            "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0017 - val_accuracy: 0.9994 - val_loss: 0.0029\n",
            "Epoch 8/10\n",
            "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0017 - val_accuracy: 0.9993 - val_loss: 0.0033\n",
            "Epoch 9/10\n",
            "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0020 - val_accuracy: 0.9994 - val_loss: 0.0031\n",
            "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9994 - loss: 0.0032\n",
            "Test Accuracy: 0.9994031190872192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test accuracy: 0.9994382262229919\n",
        "Fantastic accuracy! However lets check how many fraud detection casses were correctly identified."
      ],
      "metadata": {
        "id": "TFrPrHGpepjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An imbalanced dataset is one where the classes (in this case, fraudulent and non-fraudulent transactions) are not represented equally. This can lead to misleadingly high accuracy scores if the model simply predicts the majority class most of the time. To get a more accurate picture, we need to use metrics that account for proportion of class samples in the dataset.\n",
        "\n",
        "Here is the list of metrics that we will use:\n",
        "\n",
        "1. Confusion matrix\n",
        "               \n",
        "\n",
        "```\n",
        "                Predicted\n",
        ".................................\n",
        "      |      | True    | False\n",
        "      ...........................\n",
        "      | True |  TP     |  FN  \n",
        "   GT ...........................\n",
        "      | False|  FP     |  TN\n",
        "```\n",
        "\n",
        "TP - true positive, FN - false negative, FP - false positive, TN - true negative\n",
        "\n",
        "2. Precision = True Positives / (True Positives + False Positives)\n",
        "\n",
        "Precision is a fraction of correctly identified fraduelent transactions out of all fradulent pridictions.\n",
        "\n",
        "Precision is important because we want to avoid inconveniencing legitimate customers by mistakenly flagging their transactions as fraudulent. A high precision means fewer false alarms.\n",
        "\n",
        "3. Recall = True Positives / (True Positives + False Negatives)\n",
        "\n",
        "Recall is a fraction of correctly identified fraduelent transactions to all fradulent transactions.\n",
        "Recall is crucial because we want to catch as many fraudulent transactions as possible. A high recall means fewer fraudulent transactions go undetected.\n",
        "\n",
        "\n",
        "4. F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "\n",
        "The F1-score is a useful metric when you need to consider both precision and recall. It helps to find a model that performs well in both aspects, catching fraud while minimizing inconvenience to legitimate customers.\n",
        "\n",
        "5. AUC (Area Under the Receiver Operating Characteristic Curve)\n",
        "AUC is a measure of the model's ability to distinguish between the two classes (fraudulent and non-fraudulent).\n",
        "Interpretation: A higher AUC indicates better performance. An AUC of 0.5 represents a model that performs no better than random guessing, while an AUC of 1.0 represents a perfect model.\n",
        "\n",
        "Great explanation about AUC could be found here:\n",
        "https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
        "\n",
        "You can also see their explanation of other metrics:\n",
        "https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall\n"
      ],
      "metadata": {
        "id": "4GLDCxHmfiW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Make predictions on the test set using the model with He initialization\n",
        "y_pred = (model_with_he_init.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "# Pay attention: the order of labels is switched to make confusion matrix in the standard order\n",
        "cm = confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Calculate precision, recall, F1-score, and AUC\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "print(\"AUC:\", auc)\n"
      ],
      "metadata": {
        "id": "mgpo1sXYe8Xu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb766e0d-d8e9-4b6d-aaf7-c414eaca8e17"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
            "Confusion Matrix:\n",
            "[[   78    20]\n",
            " [   14 56850]]\n",
            "Precision: 0.8478260869565217\n",
            "Recall: 0.7959183673469388\n",
            "F1-score: 0.8210526315789474\n",
            "AUC: 0.8978360829418993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, even though the accuracy of the model is almost 100 percent the accuracy of detecting fradulent transaction is less then optimal. One sixth of fradulent prediction are not fradulent, so 14 people credit cards were frozen for no reason because of your algorithm. More then forth of fradulent transactions go undetected,so 26 people lost money because of your algorithm.\n",
        "\n",
        "**How can we do better?**\n",
        "\n",
        "One of the popular technique for dealing with unbalanced dataset is by artificially adding samples to the smaller class, or subtracting elements from the bigger class. The first type of methods are called oversampling and the second type of methods - undersampling.\n",
        "\n",
        "However you should be carefull when performing this changes to the dataset. Blindly replicating samples from smaller class, can lead to replicating noise and overfitting. Blindly redusing the bigger class can cause loss of information.\n",
        "For training neural network we will concentrait on algorithms that increase the size of the dataset.\n",
        "\n",
        "At first you will try to increase the smaller class by randomly oversampling the fradulent detection samples. Then you will try a more sofisticated oversampling algorithm SMOTE(Synthetic Minority Oversampling TEchnique). SMOTE algorithm add points by interpolating existing points in the minority(smaller) class. The next algorithm you will test is ADASYN (Adaptive Synthetic Minority Oversampling Technique). ADASYN focuses on generating data points that are harder to learn, points near the descision boundary.\n",
        "You can find more information here:\n",
        "1. https://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets\n",
        "2.\n",
        "https://imbalanced-learn.org/stable/zzz_references.html#id11\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hepUqOC-fJWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next you need to train our model on the dataset that were artificially enlarged by Naive Resampling, SMOTE and ADASYN resampling. You need to report Confusion matrix, Prescision, Recall, F1-score, and AUC. Based on this metric you  need to define the most secusfull resampling method."
      ],
      "metadata": {
        "id": "5ncEvRDB4iY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random over-sampling**"
      ],
      "metadata": {
        "id": "Y5fJymOxCjuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Apply Random Over Sampling to balance the dataset\n",
        "ros = RandomOverSampler(random_state=0)\n",
        "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "# Print the proportion of fraud/non-fraud in the resampled training set\n",
        "fraud_ratio_ros = len(y_train_resampled[y_train_resampled == 1]) / len(y_train_resampled[y_train_resampled == 0])\n",
        "print(f'After Random Over Sampling, the proportion of fraud to non-fraud transactions is {fraud_ratio_ros:.2f}')\n",
        "\n",
        "# Build the model with He Normal initialization\n",
        "model_ros = models.Sequential([\n",
        "    layers.Dense(64, input_dim=X_train.shape[1], activation='relu', kernel_initializer=initializers.HeNormal()),\n",
        "    layers.Dense(32, activation='relu', kernel_initializer=initializers.HeNormal()),\n",
        "    layers.Dense(1, activation='sigmoid')  # Sigmoid output for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_ros.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Add early stopping to monitor validation loss\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model using the resampled data\n",
        "history_ros = model_ros.fit(\n",
        "    X_train_resampled,\n",
        "    y_train_resampled,\n",
        "    epochs=10,  # Allow for more epochs to monitor overfitting\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping],  # Stop early if validation loss stops improving\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model's performance on the test data\n",
        "test_loss_ros, test_accuracy_ros = model_ros.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy (after Random Over Sampling): {test_accuracy_ros:.4f}')\n",
        "\n",
        "# Predict the outcomes on the test set\n",
        "y_pred_ros = (model_ros.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Confusion Matrix (with true positives, false positives, etc.)\n",
        "cm_ros = confusion_matrix(y_test, y_pred_ros, labels=[1, 0])\n",
        "print(\"Confusion Matrix (Random Over Sampling):\")\n",
        "print(cm_ros)\n",
        "\n",
        "# Compute Precision, Recall, F1-Score, and AUC\n",
        "precision_ros = precision_score(y_test, y_pred_ros)\n",
        "recall_ros = recall_score(y_test, y_pred_ros)\n",
        "f1_ros = f1_score(y_test, y_pred_ros)\n",
        "auc_ros = roc_auc_score(y_test, y_pred_ros)\n",
        "\n",
        "print(\"Precision (Random Over Sampling):\", precision_ros)\n",
        "print(\"Recall (Random Over Sampling):\", recall_ros)\n",
        "print(\"F1-Score (Random Over Sampling):\", f1_ros)\n",
        "print(\"AUC (Random Over Sampling):\", auc_ros)\n",
        "\n"
      ],
      "metadata": {
        "id": "dzTFcPhMuBES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "414f78ea-c28a-4fa3-8a69-3b4f9195b672"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Random Over Sampling, the proportion of fraud to non-fraud transactions is 1.00\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - accuracy: 0.9782 - loss: 0.0640 - val_accuracy: 1.0000 - val_loss: 0.0015\n",
            "Epoch 2/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step - accuracy: 0.9988 - loss: 0.0051 - val_accuracy: 0.9973 - val_loss: 0.0049\n",
            "Epoch 3/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 6.0813e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 3.5750e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 1.5661e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 3.4465e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0017 - val_accuracy: 1.0000 - val_loss: 2.4780e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0014 - val_accuracy: 1.0000 - val_loss: 1.7117e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 0.0023\n",
            "Epoch 10/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 0.0011 - val_accuracy: 0.9977 - val_loss: 0.0039\n",
            "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9990 - loss: 0.0123\n",
            "Test Accuracy (after Random Over Sampling): 0.9988\n",
            "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
            "Confusion Matrix (Random Over Sampling):\n",
            "[[   82    16]\n",
            " [   54 56810]]\n",
            "Precision (Random Over Sampling): 0.6029411764705882\n",
            "Recall (Random Over Sampling): 0.8367346938775511\n",
            "F1-Score (Random Over Sampling): 0.7008547008547008\n",
            "AUC (Random Over Sampling): 0.917892529831291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SMOTE over-sampling**"
      ],
      "metadata": {
        "id": "UBq8cnxNCwQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE  # For handling class imbalance with SMOTE\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Apply SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Print the proportion of fraud/non-fraud in the resampled training set\n",
        "fraud_ratio_smote = len(y_train_resampled[y_train_resampled == 1]) / len(y_train_resampled[y_train_resampled == 0])\n",
        "print(f'After applying SMOTE, the proportion of fraud to non-fraud transactions is {fraud_ratio_smote:.2f}')\n",
        "\n",
        "# Build the model with He Normal initialization\n",
        "model_smote = models.Sequential([\n",
        "    layers.Dense(64, input_dim=X_train.shape[1], activation='relu', kernel_initializer=initializers.HeNormal()),\n",
        "    layers.Dense(32, activation='relu', kernel_initializer=initializers.HeNormal()),\n",
        "    layers.Dense(1, activation='sigmoid')  # Sigmoid output for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_smote.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Add early stopping to monitor validation loss\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model using the resampled data\n",
        "history_smote = model_smote.fit(\n",
        "    X_train_resampled,\n",
        "    y_train_resampled,\n",
        "    epochs=10,  # Allow for more epochs to monitor overfitting\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping],  # Stop early if validation loss stops improving\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model's performance on the test data\n",
        "test_loss_smote, test_accuracy_smote = model_smote.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy (after SMOTE): {test_accuracy_smote:.4f}')\n",
        "\n",
        "# Predict the outcomes on the test set\n",
        "y_pred_smote = (model_smote.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Confusion Matrix (with true positives, false positives, etc.)\n",
        "cm_smote = confusion_matrix(y_test, y_pred_smote, labels=[1, 0])\n",
        "print(\"Confusion Matrix (SMOTE):\")\n",
        "print(cm_smote)\n",
        "\n",
        "# Compute Precision, Recall, F1-Score, and AUC\n",
        "precision_smote = precision_score(y_test, y_pred_smote)\n",
        "recall_smote = recall_score(y_test, y_pred_smote)\n",
        "f1_smote = f1_score(y_test, y_pred_smote)\n",
        "auc_smote = roc_auc_score(y_test, y_pred_smote)\n",
        "\n",
        "print(\"Precision (SMOTE):\", precision_smote)\n",
        "print(\"Recall (SMOTE):\", recall_smote)\n",
        "print(\"F1-Score (SMOTE):\", f1_smote)\n",
        "print(\"AUC (SMOTE):\", auc_smote)\n",
        "\n"
      ],
      "metadata": {
        "id": "siOtQJ-i43Nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "363f9114-fcea-4750-a75b-a427110fe81a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After applying SMOTE, the proportion of fraud to non-fraud transactions is 1.00\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - accuracy: 0.9802 - loss: 0.0572 - val_accuracy: 0.9967 - val_loss: 0.0122\n",
            "Epoch 2/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2ms/step - accuracy: 0.9986 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
            "Epoch 3/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2ms/step - accuracy: 0.9990 - loss: 0.0042 - val_accuracy: 0.9998 - val_loss: 0.0014\n",
            "Epoch 4/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 7.6891e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0024 - val_accuracy: 0.9999 - val_loss: 9.0534e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 4.0573e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0017 - val_accuracy: 1.0000 - val_loss: 9.1277e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 0.0015 - val_accuracy: 1.0000 - val_loss: 3.1446e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 6.2875e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 1.5014e-04\n",
            "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9989 - loss: 0.0172\n",
            "Test Accuracy (after SMOTE): 0.9989\n",
            "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
            "Confusion Matrix (SMOTE):\n",
            "[[   83    15]\n",
            " [   50 56814]]\n",
            "Precision (SMOTE): 0.6240601503759399\n",
            "Recall (SMOTE): 0.8469387755102041\n",
            "F1-Score (SMOTE): 0.7186147186147186\n",
            "AUC (SMOTE): 0.9230297422852091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ADASYN over-sampling**"
      ],
      "metadata": {
        "id": "0IVhTjA8C9Ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import ADASYN  # For handling class imbalance with ADASYN\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Apply ADASYN (Adaptive Synthetic Sampling) to balance the dataset\n",
        "adasyn = ADASYN(random_state=42)\n",
        "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
        "\n",
        "# Print the proportion of fraud/non-fraud in the resampled training set\n",
        "fraud_ratio_adasyn = len(y_train_resampled[y_train_resampled == 1]) / len(y_train_resampled[y_train_resampled == 0])\n",
        "print(f'After applying ADASYN, the proportion of fraud to non-fraud transactions is {fraud_ratio_adasyn:.2f}')\n",
        "\n",
        "# Build the model with He Normal initialization\n",
        "model_adasyn = models.Sequential([\n",
        "    layers.Dense(64, input_dim=X_train.shape[1], activation='relu', kernel_initializer=initializers.HeNormal()),\n",
        "    layers.Dense(32, activation='relu', kernel_initializer=initializers.HeNormal()),\n",
        "    layers.Dense(1, activation='sigmoid')  # Sigmoid output for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_adasyn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Add early stopping to monitor validation loss\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model using the resampled data\n",
        "history_adasyn = model_adasyn.fit(\n",
        "    X_train_resampled,\n",
        "    y_train_resampled,\n",
        "    epochs=10,  # Allow for more epochs to monitor overfitting\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping],  # Stop early if validation loss stops improving\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model's performance on the test data\n",
        "test_loss_adasyn, test_accuracy_adasyn = model_adasyn.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy (after ADASYN): {test_accuracy_adasyn:.4f}')\n",
        "\n",
        "# Predict the outcomes on the test set\n",
        "y_pred_adasyn = (model_adasyn.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Confusion Matrix (with true positives, false positives, etc.)\n",
        "cm_adasyn = confusion_matrix(y_test, y_pred_adasyn, labels=[1, 0])\n",
        "print(\"Confusion Matrix (ADASYN):\")\n",
        "print(cm_adasyn)\n",
        "\n",
        "# Compute Precision, Recall, F1-Score, and AUC\n",
        "precision_adasyn = precision_score(y_test, y_pred_adasyn)\n",
        "recall_adasyn = recall_score(y_test, y_pred_adasyn)\n",
        "f1_adasyn = f1_score(y_test, y_pred_adasyn)\n",
        "auc_adasyn = roc_auc_score(y_test, y_pred_adasyn)\n",
        "\n",
        "print(\"Precision (ADASYN):\", precision_adasyn)\n",
        "print(\"Recall (ADASYN):\", recall_adasyn)\n",
        "print(\"F1-Score (ADASYN):\", f1_adasyn)\n",
        "print(\"AUC (ADASYN):\", auc_adasyn)\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2QNPbusuvrc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "352fb6ec-f3f7-4f99-8fec-4bafefc30139"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After applying ADASYN, the proportion of fraud to non-fraud transactions is 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - accuracy: 0.9743 - loss: 0.0704 - val_accuracy: 0.9702 - val_loss: 0.1202\n",
            "Epoch 2/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.9988 - loss: 0.0054 - val_accuracy: 0.9581 - val_loss: 0.2097\n",
            "Epoch 3/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0035 - val_accuracy: 0.9643 - val_loss: 0.2012\n",
            "Epoch 4/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0033 - val_accuracy: 0.9717 - val_loss: 0.1805\n",
            "Epoch 5/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0027 - val_accuracy: 0.9744 - val_loss: 0.1125\n",
            "Epoch 6/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0016 - val_accuracy: 0.9738 - val_loss: 0.0957\n",
            "Epoch 7/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.9995 - loss: 0.0021 - val_accuracy: 0.9580 - val_loss: 0.1764\n",
            "Epoch 8/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 0.0015 - val_accuracy: 0.9645 - val_loss: 0.1422\n",
            "Epoch 9/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0015 - val_accuracy: 0.9847 - val_loss: 0.0788\n",
            "Epoch 10/10\n",
            "\u001b[1m11373/11373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0016 - val_accuracy: 0.9618 - val_loss: 0.1693\n",
            "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9989 - loss: 0.0172\n",
            "Test Accuracy (after ADASYN): 0.9989\n",
            "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
            "Confusion Matrix (ADASYN):\n",
            "[[   82    16]\n",
            " [   47 56817]]\n",
            "Precision (ADASYN): 0.6356589147286822\n",
            "Recall (ADASYN): 0.8367346938775511\n",
            "F1-Score (ADASYN): 0.7224669603524229\n",
            "AUC (ADASYN): 0.917954080197076\n"
          ]
        }
      ]
    }
  ]
}